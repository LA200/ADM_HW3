{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Get the list of books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as soup\n",
    "import requests\n",
    "import time\n",
    "import urllib.request\n",
    "import urllib.error as uer\n",
    "from pathlib import Path\n",
    "from lxml import etree, html\n",
    "import regex as re\n",
    "import csv\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "import io \n",
    "import codecs\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import stem\n",
    "from string import punctuation\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import ast\n",
    "from collections import Counter\n",
    "import heapq\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking all web pages of books' rank\n",
    "#https://www.goodreads.com/list/show/1.Best_Books_Ever?page=1\n",
    "\n",
    "url_rank = []\n",
    "for i in tqdm(range(1,301)):\n",
    "    url_rank.append('https://www.goodreads.com/list/show/1.Best_Books_Ever?page={}'.format(i))\n",
    "\n",
    "string_for_parsing = 'https://www.goodreads.com' #url of the site\n",
    "\n",
    "link_to_download = []\n",
    "for num_page in tqdm(url_rank):\n",
    "    book_prova  = requests.get(num_page)\n",
    "    page = soup(book_prova.text, 'html.parser')\n",
    "    domenica = page.find_all('a', {'class' : 'bookTitle'})\n",
    "    for item in domenica:\n",
    "        link_to_download.append(string_for_parsing+item['href'])\n",
    "\n",
    "#write txt file \n",
    "with open('urls.txt', 'w') as f:\n",
    "    for item in tqdm(link_to_download):\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Crawl books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"directory\").mkdir(exist_ok=True)\n",
    "\n",
    "for i in range(1,4):\n",
    "    nomeCartella = 'cartella{}'.format(i)\n",
    "    Path(nomeCartella).mkdir(exist_ok=True)\n",
    "    \n",
    "def download_html():\n",
    "    id = int(input())  #we split download in 3 parts to work in parallel\n",
    "                        #Luca = 1  #Mert = 2  #Nello = 3\n",
    "    if id ==1:\n",
    "        a = 0\n",
    "        b = 10000\n",
    "        i = 0\n",
    "    elif id ==2:\n",
    "        a = 11900\n",
    "        b = 20000\n",
    "        i = 99\n",
    "    elif id == 3:\n",
    "        a = 20000\n",
    "        b = 30000\n",
    "        i = 199\n",
    "    \n",
    "    for url in range(a,b):\n",
    "        \n",
    "        if url == 0 or url%100 == 0:\n",
    "            i += 1\n",
    "            folder_name = 'html_books_{}'.format(i)\n",
    "            Path(folder_name).mkdir(exist_ok=True)\n",
    "        urllib.request.urlretrieve(link_to_download[url], 'html_books_{}/article_{}.html'.format(i,url+1))\n",
    "        \n",
    "download_html()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsv_writer():\n",
    "    \n",
    "    print('Insert Path ')\n",
    "    \n",
    "    path = input()\n",
    "    \n",
    "    print('Insert directory')\n",
    "    \n",
    "    directory = input()\n",
    "    \n",
    "    count_error = []\n",
    "    \n",
    "    for j in range(0,201):\n",
    "        a = 1 + 100*(j)\n",
    "        b = 101 + 100*(j)\n",
    "        for k in range(a,b):\n",
    "            \n",
    "            try:\n",
    "\n",
    "                HtmlFile = open(path+'/'+directory+str(j)+'/article_{}.html'.format(k), 'r', encoding='utf-8')\n",
    "\n",
    "                source_code = HtmlFile.read()\n",
    "\n",
    "                book = soup(source_code, 'html.parser')\n",
    "\n",
    "                book_title = book.find('h1',{'id':'bookTitle'}).contents[0].strip()\n",
    "\n",
    "                book_series = book.find('h2',{'id':'bookSeries'})\n",
    "\n",
    "                if book_series.text.strip() == '':\n",
    "                    series = ''\n",
    "                else:\n",
    "                    series = book_series.contents[1].contents[0].strip()\n",
    "\n",
    "                author = book.find('span',{'itemprop':'name'}).contents[0]\n",
    "\n",
    "                rating = float(book.find('span',{'itemprop':'ratingValue'}).contents[0].strip())\n",
    "\n",
    "                number_of_rating = int(book.find('meta',{'itemprop':'ratingCount'}).contents[0].split()[0].replace(',',''))\n",
    "\n",
    "                review_count = int(book.find('meta',{'itemprop':'reviewCount'})['content'])\n",
    "\n",
    "                number_of_pages = int(book.find('span',{'itemprop':'numberOfPages'}).contents[0].split()[0])\n",
    "\n",
    "                publishing_date = book.find('div',{'class':'row'}).find_next_sibling().contents[0].split()[1:4]\n",
    "\n",
    "                plot = book.find('div',{'id':'description'})\n",
    "                \n",
    "                try:                \n",
    "                \n",
    "                    complete_plot = plot.find_all('span')\n",
    "                    \n",
    "                    if detect(str(complete_plot)) != 'en':\n",
    "                        continue\n",
    "                \n",
    "                except: \n",
    "                    \n",
    "                    complete_plot = ''\n",
    "                    \n",
    "\n",
    "                details = book.find('div',{'id':'bookDataBox'})\n",
    "\n",
    "                places = details.find_all('a', {'href': re.compile(r'/places')})\n",
    "\n",
    "                settings = ''\n",
    "                for item in places:\n",
    "                    settings += item.text+','\n",
    "\n",
    "\n",
    "                character_sw = details.find_all('a', {'href': re.compile(r'/characters')})\n",
    "\n",
    "                character = ''\n",
    "                for item in character_sw:\n",
    "                    character+= item.text+','\n",
    "            \n",
    "            except:\n",
    "                \n",
    "                count_error.append((a,j))\n",
    "                \n",
    "                continue\n",
    "\n",
    "\n",
    "            with open('books_tsv/article_{}.tsv'.format(k), 'wt', encoding=\"utf-8\") as out_file:\n",
    "                    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "                    tsv_writer.writerow(['book_title','book_series','author','rating','number_of_rating',\n",
    "                                         'review_count','number_of_pages','publishing_date','complete_plot','places','characters'])\n",
    "                    tsv_writer.writerow([book_title,series,author,rating,number_of_rating,review_count,number_of_pages,\n",
    "                                        publishing_date,complete_plot,settings,character])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert Path \n",
      "books\n",
      "Insert directory\n",
      "html_books_\n"
     ]
    }
   ],
   "source": [
    "tsv_writer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine\n",
    "\n",
    "Now, we want to create two different Search Engines that, given as input a query, return the books that match the query.\n",
    "\n",
    "First, we have pre-processed all the information collected for each book by\n",
    "\n",
    "* Removing stopwords\n",
    "* Removing punctuation\n",
    "* Stemming\n",
    "\n",
    "### 2.1. Conjunctive query\n",
    "\n",
    "#### 2.1.1) Create your index!\n",
    "\n",
    "Before building the index,\n",
    "\n",
    "We have created a file named vocabulary, in the .csv format, that maps each word to an integer (term_id).\n",
    "Then, the first brick of the homework is to create the Inverted Index. It will be a dictionary of this format:\n",
    "\n",
    "{\n",
    "\n",
    "term_id_1:[document_1, document_2, document_4],\n",
    "\n",
    "term_id_2:[document_1, document_3, document_5, document_6],\n",
    "\n",
    "...}\n",
    "\n",
    "where document_i is the id of a document that contains the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):  #this functions does the preprocessing work on the string given as input and returns the cleaned string\n",
    "\n",
    "    words = re.split(r'\\W+', str(text))\n",
    "    words = [word.lower() for word in words]\n",
    "    without_punct = [wp for wp in words if wp not in punctuation]\n",
    "    sw = stopwords.words('english')\n",
    "    without_sw = [w for w in without_punct if w not in sw] \n",
    "    list_to_remove = ['b','br','span', 'one' , 'id', 'none' ]\n",
    "    clean_more = [w for w in without_sw if w not in list_to_remove] \n",
    "    clean_more_2 = [w for w in clean_more if  not (re.findall(re.compile(r'freetext'),w)\n",
    "                                                   or re.findall(re.compile(r'\\d'),w)) ]\n",
    "    \n",
    "    \n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed_list=[ps.stem(w) for w in clean_more_2 ]\n",
    "    return ' '.join(stemmed_list)\n",
    "\n",
    "for books in range(20001,30001):\n",
    "    text_1=''\n",
    "    try:\n",
    "        df = pd.read_csv(\"books_3_tsv/article_\"+str(books)+\".tsv\", sep='\\t', encoding='utf-8' ) #vocab read csv files\n",
    "    except:\n",
    "        continue\n",
    "    df = df.fillna(\"\")\n",
    "    df['book_title']=df['book_title'].apply(cleaner)\n",
    "    df['complete_plot']=df['complete_plot'].apply(cleaner)\n",
    "    #df['book_series']=df['book_series'].apply(cleaner)\n",
    "\n",
    "    #df.to_csv('cleaned_csv/prova_libro_'+str(books)+'.csv') #to create new cleaned csv use this line \n",
    "    \n",
    "    #now create the vocab\n",
    "    \n",
    "    title = df.iloc[0, 0]\n",
    "    plot = df.iloc[0, 8]\n",
    "    text_1 = title + \" \" + plot\n",
    "    arr_1=(text_1.split())\n",
    "    c = Counter(arr_1)  # create a dictionary with the number of occurences for each word\n",
    "    file = open(\"txt_files_2/article_\"+str(books)+\".txt\", \"w\")  # save it in a txt files \n",
    "    file.write(str(c)) \n",
    "    file.close() \n",
    "\n",
    "#save vocabulary as csv with indices for each vocab\n",
    "arr = set(text.split()) # use set to eliminate repeating words\n",
    "vocab = pd.DataFrame(arr)\n",
    "vocab.to_csv(\"vocab.csv\") #save vocabulary as csv with indices for each vocab\n",
    "\n",
    "df_voc= pd.read_csv('vocab.csv')\n",
    "df_voc.columns=['id','word']\n",
    "\n",
    "from collections import defaultdict\n",
    "d = defaultdict(set)\n",
    "import ast\n",
    "\n",
    "def extract_words_2(id):\n",
    "    try:        \n",
    "        file = open(\"txt_files/article_\"+str(doc)+ \".txt\", \"r\") \n",
    "        test=ast.literal_eval(file.read())\n",
    "        file.close()\n",
    "        return test\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-3. Define a new score!\n",
    "\n",
    "As it has been requested we have added a new filter. We are asking to user to enter the query and the minimum rating for the books. We have defined a new score which is taking the rating into account and normalize it then multiply by cosine similarity. Then we are showing the 10 books with the highest new score that has been defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):\n",
    "\n",
    "    words = re.split(r'\\W+', str(text))\n",
    "    words = [word.lower() for word in words]\n",
    "    without_punct = [wp for wp in words if wp not in punctuation]\n",
    "    sw = stopwords.words('english')\n",
    "    without_sw = [w for w in without_punct if w not in sw] \n",
    "    list_to_remove = ['b','br','span', 'one' , 'id', 'none' ]\n",
    "    clean_more = [w for w in without_sw if w not in list_to_remove] \n",
    "    clean_more_2 = [w for w in clean_more if  not (re.findall(re.compile(r'freetext'),w)\n",
    "                                                   or re.findall(re.compile(r'\\d'),w)) ]\n",
    "    \n",
    "    \n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed_list=[ps.stem(w) for w in clean_more_2 ]\n",
    "    return ' '.join(stemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_link(index):\n",
    "    ind=str(index)\n",
    "    \n",
    "    folder = math.floor((index-1)/100)    \n",
    "    \n",
    "    \n",
    "    HtmlFile = open('books/html_books_'+str(folder)+'/article_{}.html'.format(ind), 'r', encoding='utf-8')\n",
    "    source_code = HtmlFile.read()\n",
    "\n",
    "    book = soup(source_code, 'html.parser')\n",
    "\n",
    "    book_link = book.find('link')\n",
    "    \n",
    "    return book_link['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:06<00:00,  4.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a csv files for all books with cleaned book title, rating, cleaned plot, url, empty similarity and score column.\n",
    "# The similarity and new score that has been defined will be added to dataframe later.\n",
    "\n",
    "df_new = pd.DataFrame(columns = ['Book Title','Rating', 'Number_of_Rating', 'Plot', 'Url', 'Similarity', 'Score'])\n",
    "title_list = []\n",
    "plot_list = []\n",
    "url_list = []\n",
    "rating_list = []\n",
    "num_rating_list = []\n",
    "\n",
    "for index in tqdm(range(1,30)):\n",
    "    try:\n",
    "        df = pd.read_csv(\"books_tsv/article_\"+str(index)+\".tsv\", sep='\\t', encoding='utf-8' )\n",
    "    except:\n",
    "        continue\n",
    "    df['book_title'] = df['book_title'].apply(cleaner)\n",
    "    title = df['book_title'].iloc[0]\n",
    "    df['complete_plot']=df['complete_plot'].apply(cleaner)\n",
    "    plot = df['complete_plot'].iloc[0]\n",
    "    complete_plt= title + \" \" + plot\n",
    "    plot_split = re.split(r'\\W+', str(complete_plt))\n",
    "    rate = df['rating'].iloc[0]/5 # Normalize the rating\n",
    "    num_rating = df['number_of_rating'].iloc[0]\n",
    "    \n",
    "    url=find_link(index)    \n",
    "    title_list.append(title)\n",
    "    plot_list.append(' '.join(plot_split))\n",
    "    url_list.append(url)\n",
    "    rating_list.append(rate)\n",
    "    num_rating_list.append(num_rating)\n",
    "    \n",
    "df_new['Book Title'] = title_list\n",
    "df_new['Plot'] = plot_list\n",
    "df_new['Url'] = url_list\n",
    "df_new['Rating'] = rating_list\n",
    "df_new['Number_of_Rating'] = num_rating_list\n",
    "df_new.to_csv ('tfidf_cos2.csv', index = False, header=True)\n",
    "\n",
    "df_new = pd.read_csv(\"tfidf_cos2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search something: last freedom\n",
      "Minimum rating: 3.5\n",
      "Minimum number of rating: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 29/29 [00:00<00:00, 427.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book Title</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>Score</th>\n",
       "      <th>Number_of_Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hitchhik guid galaxi</td>\n",
       "      <td>0.844</td>\n",
       "      <td>hitchhik guid galaxi second earth demolish mak...</td>\n",
       "      <td>https://www.goodreads.com/book/show/386162.The...</td>\n",
       "      <td>0.100256</td>\n",
       "      <td>0.084616</td>\n",
       "      <td>1446532.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>lord fli</td>\n",
       "      <td>0.738</td>\n",
       "      <td>lord fli dawn next world war plane crash uncha...</td>\n",
       "      <td>https://www.goodreads.com/book/show/7624.Lord_...</td>\n",
       "      <td>0.093382</td>\n",
       "      <td>0.068916</td>\n",
       "      <td>2274444.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>anim farm</td>\n",
       "      <td>0.790</td>\n",
       "      <td>anim farm librarian note altern cover edit edi...</td>\n",
       "      <td>https://www.goodreads.com/book/show/170448.Ani...</td>\n",
       "      <td>0.038673</td>\n",
       "      <td>0.030552</td>\n",
       "      <td>2759761.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hunger game</td>\n",
       "      <td>0.866</td>\n",
       "      <td>hunger game could surviv wild everi make sure ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2767052-th...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6411305.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>harri potter order phoenix</td>\n",
       "      <td>0.900</td>\n",
       "      <td>harri potter order phoenix door end silent cor...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2.Harry_Po...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2526182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kill mockingbird</td>\n",
       "      <td>0.856</td>\n",
       "      <td>kill mockingbird unforgett novel childhood sle...</td>\n",
       "      <td>https://www.goodreads.com/book/show/2657.To_Ki...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4529336.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pride prejudic</td>\n",
       "      <td>0.852</td>\n",
       "      <td>pride prejudic altern cover edit isbn sinc imm...</td>\n",
       "      <td>https://www.goodreads.com/book/show/1885.Pride...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3019217.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twilight</td>\n",
       "      <td>0.720</td>\n",
       "      <td>twilight three thing absolut posit first edwar...</td>\n",
       "      <td>https://www.goodreads.com/book/show/41865.Twil...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4991938.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>book thief</td>\n",
       "      <td>0.874</td>\n",
       "      <td>book thief librarian note altern cover edit fo...</td>\n",
       "      <td>https://www.goodreads.com/book/show/19063.The_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1846649.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>chronicl narnia</td>\n",
       "      <td>0.852</td>\n",
       "      <td>chronicl narnia journey end world fantast crea...</td>\n",
       "      <td>https://www.goodreads.com/book/show/11127.The_...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>522051.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Book Title  Rating  \\\n",
       "11        hitchhik guid galaxi   0.844   \n",
       "22                    lord fli   0.738   \n",
       "6                    anim farm   0.790   \n",
       "0                  hunger game   0.866   \n",
       "1   harri potter order phoenix   0.900   \n",
       "2             kill mockingbird   0.856   \n",
       "3               pride prejudic   0.852   \n",
       "4                     twilight   0.720   \n",
       "5                   book thief   0.874   \n",
       "7              chronicl narnia   0.852   \n",
       "\n",
       "                                                 Plot  \\\n",
       "11  hitchhik guid galaxi second earth demolish mak...   \n",
       "22  lord fli dawn next world war plane crash uncha...   \n",
       "6   anim farm librarian note altern cover edit edi...   \n",
       "0   hunger game could surviv wild everi make sure ...   \n",
       "1   harri potter order phoenix door end silent cor...   \n",
       "2   kill mockingbird unforgett novel childhood sle...   \n",
       "3   pride prejudic altern cover edit isbn sinc imm...   \n",
       "4   twilight three thing absolut posit first edwar...   \n",
       "5   book thief librarian note altern cover edit fo...   \n",
       "7   chronicl narnia journey end world fantast crea...   \n",
       "\n",
       "                                                  Url  Similarity     Score  \\\n",
       "11  https://www.goodreads.com/book/show/386162.The...    0.100256  0.084616   \n",
       "22  https://www.goodreads.com/book/show/7624.Lord_...    0.093382  0.068916   \n",
       "6   https://www.goodreads.com/book/show/170448.Ani...    0.038673  0.030552   \n",
       "0   https://www.goodreads.com/book/show/2767052-th...    0.000000  0.000000   \n",
       "1   https://www.goodreads.com/book/show/2.Harry_Po...    0.000000  0.000000   \n",
       "2   https://www.goodreads.com/book/show/2657.To_Ki...    0.000000  0.000000   \n",
       "3   https://www.goodreads.com/book/show/1885.Pride...    0.000000  0.000000   \n",
       "4   https://www.goodreads.com/book/show/41865.Twil...    0.000000  0.000000   \n",
       "5   https://www.goodreads.com/book/show/19063.The_...    0.000000  0.000000   \n",
       "7   https://www.goodreads.com/book/show/11127.The_...    0.000000  0.000000   \n",
       "\n",
       "    Number_of_Rating  \n",
       "11         1446532.0  \n",
       "22         2274444.0  \n",
       "6          2759761.0  \n",
       "0          6411305.0  \n",
       "1          2526182.0  \n",
       "2          4529336.0  \n",
       "3          3019217.0  \n",
       "4          4991938.0  \n",
       "5          1846649.0  \n",
       "7           522051.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the tfidf and cosine similarity score for all books with the query\n",
    "\n",
    "vectorizer = TfidfVectorizer() # Get tf-idf matrix using fit_transform function\n",
    "X = vectorizer.fit_transform(df_new['Plot'].values.astype('U')) # Store tf-idf representations of all docs\n",
    "\n",
    "query = str(input('Search something: ')) # Ask the user to write the query\n",
    "min_rating = np.float(input('Minimum rating: ')) # We have asked to user to write the minimum rating he/she wants\n",
    "min_number_rating = int(input('Minimum number of rating: ')) # We have asked to user to write the minimum rating he/she wants\n",
    "\n",
    "query_vec = vectorizer.transform([query]) # Ip -- (n_docs,x), Op -- (n_docs,n_Feats)\n",
    "results = cosine_similarity(X,query_vec).reshape((-1,)) # Op -- (n_docs,1) -- Cosine Sim with each doc\n",
    "cos_sim = results.tolist() # Convert the cosine similarity result to list format\n",
    "df_new['Similarity'] = cos_sim\n",
    "df_new['Score'] = cos_sim * df_new['Rating']  # Calculate the score as cosine_similarity*rating and write to Score column of dataframe\n",
    "\n",
    "df_empty = pd.DataFrame(columns = ['Book Title','Rating', 'Plot', 'Url', 'Similarity','Score'])\n",
    "\n",
    "for i in tqdm(range(len(df_new))):\n",
    "    if df_new.loc[i]['Rating']*5 >= min_rating and df_new.loc[i]['Number_of_Rating'] >= min_number_rating: # Filter the books with minimum rating provided by user.\n",
    "        df_updated = pd.concat([df_empty, df_new], axis=0) # Add the book to updated dataframe which has rating greater than min rating\n",
    "\n",
    "df_updated.nlargest(10,'Score') # Take the first 10 books with larger new score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "3\n",
      "6\n",
      "9\n",
      "12\n",
      "15\n",
      "18\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
