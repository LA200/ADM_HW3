{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "import io \n",
    "import codecs\n",
    "import csv\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk import stem\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "import ast\n",
    "from collections import Counter\n",
    "\n",
    "import heapq\n",
    "import math\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaner(text):  #this functions does the preprocessing work on the string given as input and returns the cleaned string\n",
    "\n",
    "    words = re.split(r'\\W+', str(text))\n",
    "    words = [word.lower() for word in words]\n",
    "    without_punct = [wp for wp in words if wp not in punctuation]\n",
    "    sw = stopwords.words('english')\n",
    "    without_sw = [w for w in without_punct if w not in sw] \n",
    "    list_to_remove = ['b','br','span', 'one' , 'id', 'none' ]\n",
    "    clean_more = [w for w in without_sw if w not in list_to_remove] \n",
    "    clean_more_2 = [w for w in clean_more if  not (re.findall(re.compile(r'freetext'),w)\n",
    "                                                   or re.findall(re.compile(r'\\d'),w)) ]\n",
    "    \n",
    "    \n",
    "    ps = nltk.stem.PorterStemmer()\n",
    "    stemmed_list=[ps.stem(w) for w in clean_more_2 ]\n",
    "    return ' '.join(stemmed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for books in range(20001,30001):\n",
    "    text_1=''\n",
    "    try:\n",
    "        df = pd.read_csv(\"books_3_tsv/article_\"+str(books)+\".tsv\", sep='\\t', encoding='utf-8' ) #vocab read csv files\n",
    "    except:\n",
    "        continue\n",
    "    df = df.fillna(\"\")\n",
    "    df['book_title']=df['book_title'].apply(cleaner)\n",
    "    df['complete_plot']=df['complete_plot'].apply(cleaner)\n",
    "    #df['book_series']=df['book_series'].apply(cleaner)\n",
    "\n",
    "    #df.to_csv('cleaned_csv/prova_libro_'+str(books)+'.csv') #to create new cleaned csv use this line \n",
    "    \n",
    "    #now create the vocab\n",
    "    \n",
    "    title = df.iloc[0, 0]\n",
    "    plot = df.iloc[0, 8]\n",
    "    text_1 = title + \" \" + plot\n",
    "    arr_1=(text_1.split())\n",
    "    c = Counter(arr_1)  # create a dictionary with the number of occurences for each word\n",
    "    file = open(\"txt_files_2/article_\"+str(books)+\".txt\", \"w\")  # save it in a txt files \n",
    "    file.write(str(c)) \n",
    "    file.close() \n",
    "\n",
    " #save vocabulary as csv with indices for each vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "d = defaultdict(list)  #create a defaultdict. keys: word , values: (document id, occurrences of the word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_counter(id): #given the index of the book it reads the dictionary in txt file and returns it \n",
    "    try:\n",
    "        \n",
    "        file = open(\"txt_files_2/article_\"+str(doc)+ \".txt\", \"r\")\n",
    "        dic=file.read()\n",
    "        prova=dic.replace('Counter(','').replace(')','')\n",
    "        test=ast.literal_eval(prova) #evaluate the dictionary saved as a string in the txt file \n",
    "        file.close()\n",
    "        return test\n",
    "    except:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(20000,30001):   #fill the defaultdict d with all the documents\n",
    "    result=extract_from_counter(doc)\n",
    "    for key in result.keys():\n",
    "        heapq.heappush(d[key],(result[key],doc)) # using heap to store values and to keep the order\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_voc= pd.read_csv('vocab.csv') # read the vocabulary id: word from the csv file\n",
    "df_voc.columns=['id','word'] # change columns names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_keys=d.keys() # create a new dictionary to subsitute words with words_id\n",
    "new_dic=defaultdict(list)\n",
    "for item in dic_keys:\n",
    "    try:\n",
    "        new_key=df_voc[df_voc.word==item].id.iloc[0] # find the word id from the vocabulary\n",
    "        new_dic[new_key]=d[item] # copy the values in the new dictionary\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we collect the values to calculate the tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the lenght of each document in a dictionary\n",
    "dic_lenght=defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in range(20001,30001): # fill dic_lenght with all the documents lenghts \n",
    "    result=extract_from_counter(doc)\n",
    "    lenght=sum(result.values()) # sum the values of the Counter of the document \n",
    "    if(lenght==0):\n",
    "        continue\n",
    "    dic_lenght[doc]=lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_number=10000  # numbers of documents in the collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the number of documents for each word in a different dictionary\n",
    "documents_with_word= defaultdict(int)\n",
    "for item in d.keys():\n",
    "    documents_with_word[item]=len(d[item])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create again the dictionary with the heap ordered by tf_idf\n",
    "final_inverted_index=defaultdict(list) #this is the final inverted indev with words as keys.\n",
    "for doc in range(20000,30001):\n",
    "    result=extract_from_counter(doc)\n",
    "    doc_len=dic_lenght[doc]\n",
    "    for key in result.keys():\n",
    "        tf= result[key] / doc_len\n",
    "        idf= math.log(documents_number / documents_with_word[key] ,10)\n",
    "        tf_idf= tf * idf\n",
    "        heapq.heappush(final_inverted_index[key],(tf_idf,doc)) # the list is ordered by the tfidf using a heap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change the word with the numbers  \n",
    "dic_keys=final_inverted_index.keys()\n",
    "inverted_index_tfidf=defaultdict(list)  #final inverted index\n",
    "for item in dic_keys:\n",
    "    try:\n",
    "        new_key=df_voc[df_voc.word==item].id.iloc[0]\n",
    "        inverted_index_tfidf[new_key]=final_inverted_index[item]\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= open('inverted_index_1.txt','r') #read the inverted index of the previous search engine \n",
    "obj= file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dic=ast.literal_eval(obj) #evaluate string as dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tfidf(word_list): # given a list, this functions returns the tfidf_vector for the string\n",
    "    c=Counter(word_list)\n",
    "    res_vec=[] \n",
    "    string_len=len(word_list)\n",
    "    for word in word_list: # calculate the tfidf for all the word in the query \n",
    "        tf=c[word]/string_len\n",
    "        idf= math.log(documents_number / documents_with_word[word] ,10) \n",
    "        res_vec.append(tf*idf) \n",
    "    return res_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process the query\n",
    "def search_query(): #the search engine is similar to the previous one, but in this case we use it to filter the list of\n",
    "                    # documents that contains all the words of the query. We will apply the cosine similarity function to this books\n",
    "    query= str(input('Insert the query '))\n",
    "    final_query=cleaner(query)\n",
    "    word_list=final_query.split()\n",
    "    tf_idf=find_tfidf(word_list)\n",
    "    key_list=[]\n",
    "    for word in word_list:\n",
    "        key=df_voc[df_voc.word==word].id.iloc[0]\n",
    "        key_list.append(key)\n",
    "    try:\n",
    "        result=new_dic[key_list[0]]\n",
    "    except:\n",
    "        print('No results found for the input query')\n",
    "        return 0,0,0\n",
    "    prova=[]\n",
    "    for item in key_list:\n",
    "        result=result.intersection(new_dic[item])\n",
    "        print(item)\n",
    "    return result,key_list,tf_idf  #it returns the set of documents, the words in the query, and the tf_idf vector of the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this function is to allow the user to search for key words that could identify some books. Please avoid to insert only stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert the query hard love pain life\n",
      "2\n",
      "19024\n",
      "14338\n",
      "1699\n"
     ]
    }
   ],
   "source": [
    "res,key_list,tf_vector=search_query()   # call the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list=[] \n",
    "for book in res: # for each book in result we create the tfidf_vector. The lenght of the vector is equal to the length \n",
    "                # of the tfidf_vector of the query\n",
    "    vec=[]\n",
    "    for key in key_list:\n",
    "        data_frame= pd.DataFrame(inverted_index_tfidf[key])#convert the values of the key in a dataframe \n",
    "        data_frame.columns=['tf','doc']\n",
    "        vec.append(data_frame[data_frame.doc== book].tf.iloc[0]) # search the tfidf value filtering the dataframe \n",
    "   \n",
    "    similarity= cos_similarity(tf_vector,vec) # for each book we apply the cosine similarity\n",
    "    result_list.append((similarity, book)) # store the result in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_list=heapq.nlargest(4,result_list)  # get the top 4 results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_link(index):  # find the link to the book using its html document \n",
    "    ind=str(index)\n",
    "    if(index%100==0):\n",
    "        folder=int(ind[0:3])-1\n",
    "    else:\n",
    "        folder=ind[0:3]\n",
    "    HtmlFile = open('books_3/html_books_'+str(folder)+'/article_{}.html'.format(ind), 'r', encoding='utf-8')\n",
    "    source_code = HtmlFile.read()\n",
    "\n",
    "    book = soup(source_code, 'html.parser')\n",
    "\n",
    "    book_link = book.find('link')\n",
    "    \n",
    "    return book_link['href']  #return the link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dataframe= pd.DataFrame(columns=['Title','Plot','Url','Similarity'])\n",
    "title_list=[]\n",
    "plot_list=[]\n",
    "url_list=[]\n",
    "sim_list=[]\n",
    "for similar,index in final_list:  # for each book we print the title, the plot, the url and the cosine similarity\n",
    "    df = pd.read_csv(\"books_3_tsv/article_\"+str(index)+\".tsv\", sep='\\t', encoding='utf-8' )\n",
    "    plot_split=re.split(r'\\W+', str(df['complete_plot'].iloc[0]))\n",
    "    title=df['book_title'].iloc[0]\n",
    "    list_to_remove = ['b','br','span', 'one' , 'id', 'none','[',']']\n",
    "    clean_more = [w for w in plot_split if w not in list_to_remove] \n",
    "    clean_more_2 = [w for w in clean_more if  not (re.findall(re.compile(r'freetext'),w)\n",
    "                                                   or re.findall(re.compile(r'\\d'),w)) ]\n",
    "    url=find_link(index)\n",
    "    title_list.append(title)\n",
    "    plot_list.append(' '.join(clean_more_2))\n",
    "    url_list.append(url)\n",
    "    sim_list.append(similar)\n",
    "result_dataframe['Title'] = title_list\n",
    "result_dataframe['Plot'] = plot_list\n",
    "result_dataframe['Url'] = url_list\n",
    "result_dataframe['Similarity']=sim_list\n",
    "#print(title,' '.join(clean_more_2),url, similar)\n",
    "#print('\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Plot</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Unbearable Lightness: A Story of Loss and Gain</td>\n",
       "      <td>I didn t decide to become anorexic It snuck u...</td>\n",
       "      <td>https://www.goodreads.com/book/show/9219901-un...</td>\n",
       "      <td>0.977755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Most of All You</td>\n",
       "      <td>i A broken woman i Crystal learned long ago t...</td>\n",
       "      <td>https://www.goodreads.com/book/show/32854499-m...</td>\n",
       "      <td>0.972176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A Drop in the Ocean</td>\n",
       "      <td>On her birthday Anna Fergusson Boston neurosc...</td>\n",
       "      <td>https://www.goodreads.com/book/show/30129954-a...</td>\n",
       "      <td>0.967638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Unexpected Blessing</td>\n",
       "      <td>i This is an alternate cover edition of ISBN ...</td>\n",
       "      <td>https://www.goodreads.com/book/show/52374459-a...</td>\n",
       "      <td>0.956257</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Title  \\\n",
       "0  Unbearable Lightness: A Story of Loss and Gain   \n",
       "1                                 Most of All You   \n",
       "2                             A Drop in the Ocean   \n",
       "3                          An Unexpected Blessing   \n",
       "\n",
       "                                                Plot  \\\n",
       "0   I didn t decide to become anorexic It snuck u...   \n",
       "1   i A broken woman i Crystal learned long ago t...   \n",
       "2   On her birthday Anna Fergusson Boston neurosc...   \n",
       "3   i This is an alternate cover edition of ISBN ...   \n",
       "\n",
       "                                                 Url  Similarity  \n",
       "0  https://www.goodreads.com/book/show/9219901-un...    0.977755  \n",
       "1  https://www.goodreads.com/book/show/32854499-m...    0.972176  \n",
       "2  https://www.goodreads.com/book/show/30129954-a...    0.967638  \n",
       "3  https://www.goodreads.com/book/show/52374459-a...    0.956257  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dataframe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
